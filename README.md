# Spam_Classifier
This dataset consists of 4 columns. The first column is the serial number, second is mentioned about the classified text **“spam”** *(if it is spam)* or **“ham”** *(if not spam)*, whereas the third column is all about the text data or SMS & finally, the fourth column is filled with **0**s and **1**s *(where, 0 means not spam a.k.a. “ham” and 1 denotes “spam”)*.
For analyzing this dataset, I use *Python* programming language (*Jupyter Notebook* to be precise).
At first, I checked the shape of the dataset which turns to be *(5171,4)* means the dataset has *5171 rows* and *4 columns*. After that I checked the data types of each of the column which shows the *1st* and *4th* columns are **int64** and *2nd* & *3rd* ones are **object**. As I’ve mentioned above 1st column is no use in analyzing the data and what 2nd column describes through text 4th column shows the same as 0s and 1s. That’s why it is better to **drop the 1st and 2nd columns as a whole**.
Now I checked for the null values in the remaining two columns which shows that neither of them poses any. 
As the text data or SMS to be manipulated before feeding it any of the machine/deep learning algorithms, I divided the columns and saved them into different variables namely *‘X’* contains the *text* & *‘y’* contains the labels or *0s* and *1s*.
Now initiating a “for loop” which iterates till the last entry of ‘X’ to remove all the **special characters** present in the string and everything other than *small case **“a”** to **“z”*** and *capital **“A”** to **“Z”***, as well as converting all the letters into *small case* and splitting each of the word from sentences. Then every word has been checked whether it is present in the *stop-words* of English language, then *stemming* them and joining them together; finally, saved the processed sentences in a separate list named **“cleaned_data”**.
Moving forward, each of the text SMS have *“subject”* at the beginning, which seems to be *not useful* to predict the SMS to be spam or not spam. So, I dropped that word from each of the entry by initiating a for loop and save the output to another list named **“cleaned”**.
After manipulation of data is over, **Word Embedding** technique should be followed. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. Each word is represented by a real-valued vector, often tens or hundreds of dimensions. This is contrasted to the thousands or millions of dimensions required for sparse word representations, such as a one-hot encoding. 
To perform word embedding we need to pre-define our *vocabulary size* which in turn **4500** for this case. After applying this function to the **“cleaned”** list I get the required value and saved it in **“onehot”** variable. During this time I import ***“pad_sequence”***, it is an useful tool to make all the *sentences* (or *vectors*) of **equal size**. Two different sized sentences to be made equal by adding 0s before or after the short one. Here I used padding sequence to be **‘pre’** means *0s should be added before the sentence (or vector)* provided the sentence length be given as **30**. After applying this method on **“onehot”** I saved the output as **“emb_data”**.
Now, as the input is in shape, I created the **LSTM** model to feed the data and getting the output. For this I’ve use a *Sequential* model with an *Embedding layer*, two *Dropout layers* (to be discussed later on), one *LSTM layer* with **150 neurons**, two *Dense layers* (to be discussed later) and finally a compiler layer with the **Binary Cross-entropy** as *loss function* & **‘adam’** be the *optimizer*.
Then converting the **“emb_data”** as an numpy array and named ***“X_final”***, and do the same thing for **‘y’** and saved it as ***“y_final”***. Then the whole data to be splitted into *test* and *training* sets with test size be *35%*.
Then fitting the training data in LSTM model (that I’ve created), with *validation* being the test sets, with **40** epochs & **batch size** of **32**. And finally predicting the results.
## Dropout Layer:  
Simply put, dropout refers to ignoring units (i.e., neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, I mean these units are not considered during a particular forward or backward pass.
### Why do we need dropout at all?	 
A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.
## Dense Layer:
The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer.
In the background, the dense layer performs a matrix-vector multiplication. The values used in the matrix are actually parameters that can be trained and updated with the help of backpropagation. The output generated by the dense layer is an ‘m’ dimensional vector. Thus, dense layer is basically used for changing the dimensions of the vector. Dense layers also apply operations like rotation, scaling, translation on the vector.
